{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines for the Twitter Sentiment Analysis CIL Project\n",
    "\n",
    "They will mostly employ simple linear classifiers on naive operations on the word embeddings, such as averaging.\n",
    "\n",
    " * Attempt 1. uses word embedding vector averaging.\n",
    " * Attempt 2. uses embedding vector concatenation and is VERY memory-hungry.\n",
    " * The `del`s in the code seek to alleviate some of the memory pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.grid_search import *\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# We will work on the preprocessed data, so that we have a common ground with\n",
    "# the deep learning competitors.\n",
    "pp = os.path.join('..', 'data', 'preprocessing')\n",
    "tr = os.path.join('..', 'data', 'train')\n",
    "vc = os.path.join('..', 'data', 'word2vec')\n",
    "\n",
    "TRAINING_SAMPLES = 2500000\n",
    "MAX_TWEET_LENGHT = 25\n",
    "\n",
    "# How many Tweets to sample.\n",
    "# Averaging should work with all of them, but concatenation chokes\n",
    "# on even a tenth in its current implementation.\n",
    "LIMIT = 250000\n",
    "\n",
    "#training files\n",
    "train_neg_file = os.path.join(tr, 'train_pos_full_orig.txt')\n",
    "train_pos_file = os.path.join(tr, 'train_neg_full_orig.txt')\n",
    "\n",
    "#load word2vec model\n",
    "model = Word2Vec.load(os.path.join(vc, 'word2vec-local-gensim-orig-20.bin'))\n",
    "EMBEDDING_DIM = model.vector_size\n",
    "print(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_training_data_concat():\n",
    "    trainX = np.zeros((TRAINING_SAMPLES, MAX_TWEET_LENGHT*EMBEDDING_DIM))\n",
    "    trainY = np.ones((TRAINING_SAMPLES))\n",
    "    tweet = 0\n",
    "    for filename in [train_neg_file, train_pos_file]:\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                embedded_words = []\n",
    "                line = line.strip()\n",
    "                for word in line.split():\n",
    "                    if word in model:\n",
    "                        embedded_words.append(model[word])\n",
    "                embedded_words.append(np.zeros(MAX_TWEET_LENGHT*EMBEDDING_DIM+2))\n",
    "                tweet_embedded = np.hstack(embedded_words)\n",
    "                trainX[tweet,:] = tweet_embedded[0:MAX_TWEET_LENGHT*EMBEDDING_DIM]\n",
    "                if filename == train_neg_file:\n",
    "                    trainY[tweet] = -1\n",
    "                tweet += 1\n",
    "        print(filename)\n",
    "        print(tweet)\n",
    "    return trainX, trainY\n",
    "\n",
    "def load_training_data_average():\n",
    "    trainX = np.zeros((TRAINING_SAMPLES, EMBEDDING_DIM))\n",
    "    trainY = np.ones((TRAINING_SAMPLES))\n",
    "    tweet = 0\n",
    "    for filename in [train_neg_file, train_pos_file]:\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                embedded_words = []\n",
    "                line = line.strip()\n",
    "                for word in line.split():\n",
    "                    if word in model:\n",
    "                        embedded_words.append(model[word])\n",
    "                tweet_embedded = np.mean(embedded_words)\n",
    "                trainX[tweet,:] = tweet_embedded\n",
    "                if filename == train_neg_file:\n",
    "                    trainY[tweet] = -1\n",
    "                tweet += 1\n",
    "    return trainX, trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'alpha': [0.00001, 0.00005, 0.0001, 0.0005, 0.001],\n",
    "}\n",
    "\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=lambda x: x[1], reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"{2}: Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores),\n",
    "              i + 1))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_avg(trainX, trainY):\n",
    "    \"\"\"Averages each tweet's word vectors into one, and trains a linear classifier.\"\"\"\n",
    "\n",
    "    print(\"Evaluating input of size {0}.\".format(trainX.shape))\n",
    "    print(trainX.shape)\n",
    "    print(trainY.shape)\n",
    "    trainX, trainY = shuffle(trainX, trainY)\n",
    "    \n",
    "    gs = GridSearchCV(SGDClassifier(), grid, cv=3, verbose=True)\n",
    "    print(\"Starting grid search...\")\n",
    "    res = gs.fit(trainX, trainY)\n",
    "    report(res.grid_scores_, n_top=25)\n",
    "    \n",
    "    predY = res.predict(trainX)\n",
    "    acc = accuracy_score(trainY, predY)\n",
    "    f1 = accuracy_score(trainY, predY)\n",
    "    \n",
    "    print(\"Train accuracy: {0}\\nTrain F1 score: {1}\".format(acc, f1))\n",
    "    \n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_concat(trainX, trainY):\n",
    "    \"\"\"Concatenates each tweet's word vectors into one, and trains a linear classifier.\n",
    "    \n",
    "    Note: can get VERY memory-hungry.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Evaluating input of size {0}.\".format(trainX.shape))\n",
    "    \n",
    "    trainX, trainY = shuffle(trainX, trainY)\n",
    "    \n",
    "    gs = GridSearchCV(SGDClassifier(), grid, cv=2, verbose=True)\n",
    "    print(\"Starting grid search...\")\n",
    "    res = gs.fit(trainX, trainY)\n",
    "    report(res.grid_scores_, n_top=25)\n",
    "    \n",
    "    predY = res.predict(trainX)\n",
    "    acc = accuracy_score(trainY, predY)\n",
    "    f1 = accuracy_score(trainY, predY)\n",
    "    \n",
    "    print(\"Train accuracy: {0}\\nTrain F1 score: {1}\".format(acc, f1))\n",
    "    \n",
    "    return res   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train/train_pos_full_orig.txt\n",
      "1250000\n",
      "../data/train/train_neg_full_orig.txt\n",
      "2500000\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = load_training_data_concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating input of size (2500000, 500).\n",
      "Starting grid search...\n",
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 19.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Mean validation score: 0.677 (std: 0.003)\n",
      "Parameters: {'alpha': 0.001}\n",
      "\n",
      "2: Mean validation score: 0.665 (std: 0.005)\n",
      "Parameters: {'alpha': 0.0005}\n",
      "\n",
      "3: Mean validation score: 0.655 (std: 0.001)\n",
      "Parameters: {'alpha': 0.0001}\n",
      "\n",
      "4: Mean validation score: 0.644 (std: 0.004)\n",
      "Parameters: {'alpha': 1e-05}\n",
      "\n",
      "5: Mean validation score: 0.632 (std: 0.010)\n",
      "Parameters: {'alpha': 5e-05}\n",
      "\n",
      "Train accuracy: 0.6919524\n",
      "Train F1 score: 0.6919524\n"
     ]
    }
   ],
   "source": [
    "concat_res = eval_concat(np.nan_to_num(trainX), trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating input of size (2500000, 20).\n",
      "(2500000, 20)\n",
      "(2500000,)\n",
      "Starting grid search...\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   35.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Mean validation score: 0.634 (std: 0.001)\n",
      "Parameters: {'alpha': 1e-05}\n",
      "\n",
      "2: Mean validation score: 0.632 (std: 0.001)\n",
      "Parameters: {'alpha': 0.001}\n",
      "\n",
      "3: Mean validation score: 0.631 (std: 0.000)\n",
      "Parameters: {'alpha': 5e-05}\n",
      "\n",
      "4: Mean validation score: 0.631 (std: 0.001)\n",
      "Parameters: {'alpha': 0.0005}\n",
      "\n",
      "5: Mean validation score: 0.631 (std: 0.000)\n",
      "Parameters: {'alpha': 0.0001}\n",
      "\n",
      "Train accuracy: 0.6313264\n",
      "Train F1 score: 0.6313264\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = load_training_data_average()\n",
    "avg_res = eval_avg(np.nan_to_num(trainX), trainY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
